{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[:+:] Feature Selection + Nearest Neighbor [:+:]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "#import seaborn as seab\n",
    "from itertools import chain, combinations\n",
    "#from scipy.spatial import minkowski_distance\n",
    "from numba import jit, cuda\n",
    "from timeit import default_timer as timer\n",
    "# from itertools import combinations\n",
    "\n",
    "# [:::] --- TodDo  ---\n",
    "# - [x] Find \"default Rate\" : size(most common class)/size(dataset)\n",
    "# - [x] Create Minkowski Distance function for two points\n",
    "# - [x] Find nn of a single data point using only one feature\n",
    "# - [x] Get rid of spurrious percision\n",
    "# - [x] For small data, find nn of every data point using one feature + max accuracy\n",
    "# - [x] For small data, find nn of every data point using all features + max accuracy\n",
    "# - [x] Complete Forward selection for small datas set\n",
    "# - [x] GPU Multithreading for Classifier\n",
    "# - [x] Leave-One-Out Cross Validation\n",
    "# - [] Complete Backward Elimination for small data set\n",
    "# - [] Complete Both selection methods for large data set\n",
    "# - [] multithread everything else\n",
    "# - [] Data Viz\n",
    "# - [] Report\n",
    "# - [] Find more brown m&ms\n",
    "# - [] Clean up the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[+] Reading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 4 entries of the first column: data[colIdx][rowIdx]:\n",
      "0    2.0\n",
      "1    1.0\n",
      "2    2.0\n",
      "3    2.0\n",
      "4    2.0\n",
      "Name: 0, dtype: float64\n",
      "<class 'numpy.float64'>\n",
      "\n",
      "Original Matrix Dimensions:\n",
      "(500, 7)\n",
      "Data Head:\n",
      "     0         1         2         3         4         5         6\n",
      "0  2.0 -1.101866 -0.782026  0.552502  0.454685  1.132363  1.135458\n",
      "1  1.0  0.928979 -0.169694  1.465293 -1.591929  0.144808  0.162709\n",
      "2  2.0  1.123118 -1.384730 -0.903598  0.692522  0.669263 -0.142156\n",
      "3  2.0  0.816617 -0.043628  1.026966  0.231013 -0.006551  2.316509\n",
      "4  2.0 -1.159129 -1.341375  0.459997  0.631261 -1.479455  0.520158\n",
      "MetaData: \n",
      "Occurrence counts of classes:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0\n",
       "1.0     95\n",
       "2.0    405\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convert to numpy array to enable multithreading later:\n",
      "column size after conversion: 7\n",
      "column size after transposition: 500\n",
      "Dimensions after transposition: (7, 500)\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "#Read data from the text files and set as a pandas data frame\n",
    "data = pd.read_csv('CS170_Small_Data__19.txt', sep=\"  \", engine='python', header=None)\n",
    "\n",
    "\n",
    "#print all the data:\n",
    "#print(lilData)\n",
    "\n",
    "\n",
    "#----print just the 'label' column (and indeces):\n",
    "print(\"The first 4 entries of the first column: data[colIdx][rowIdx]:\")\n",
    "print(data[0][0:5])\n",
    "print(type(data[0][0]))\n",
    "\n",
    "\n",
    "print(\"\\nOriginal Matrix Dimensions:\")\n",
    "lil_rowXcol= data.shape\n",
    "print(lil_rowXcol)\n",
    "\n",
    "\n",
    "print(\"Data Head:\")\n",
    "print(data.head())\n",
    "\n",
    "print(\"MetaData: \")\n",
    "data.describe()\n",
    "\n",
    "\n",
    "\n",
    "print('Occurrence counts of classes:')\n",
    "# count occurrences of the class column\n",
    "occur = data.groupby([0]).size()\n",
    "# display occurrences of a particular column\n",
    "display(occur)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nConvert to numpy array to enable multithreading later:\")\n",
    "data= data.to_numpy()\n",
    "print(\"column size after conversion: \" + str(len(data[0])))\n",
    "data= data.transpose()\n",
    "print(\"column size after transposition: \" + str(len(data[0])))\n",
    "print(\"Dimensions after transposition: \" + str(data.shape))\n",
    "\n",
    "print(type(data[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[+] Minkowski Distance Calculation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My (from-scratch)Function takes in indices of two data points and exponent p, returns the minkowski distance between them based on the four dimensions and the exponent\n",
    "@jit(target_backend='cuda')\n",
    "def minkowski(data,rowIdx1,rowIdx2,ftSet,p):\n",
    "    # featSet1= []\n",
    "    # featSet2= []\n",
    "    sigma= 0\n",
    "    for ft in ftSet:\n",
    "        # featSet1.append(data[ft][rowIdx1])\n",
    "        # featSet2.append(data[ft][rowIdx2])\n",
    "        sigma+= ((((data[ft][rowIdx1]-data[ft][rowIdx2])**2))**(1/2))  **(p)\n",
    "        distance= sigma**(1/p)\n",
    "    return distance\n",
    "    # return distance, featSet1, featSet2\n",
    "\n",
    "# d= minkowski(lilData, rowIdx1=1, rowIdx2= 2, ftSet=(1,2,3,4,5,6), p=2)\n",
    "# print(d)\n",
    "# minkowski_distance(d[1],d[2],p=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[:+:] Forward Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[::] -- Backward Elimination Search Tree Level 0 -- [::]\n",
      "        - Testing with features [1 2 3 4 5 6] | Accuracy: %81.6\n",
      "[+] -- On level 0, we use feature subset of size 6: [1 2 3 4 5 6] with an accuracy of %81.6\n",
      "\n",
      "[::] -- Backward Elimination Search Tree Level 1 -- [::]\n",
      "        - Testing with features [1 2 3 4 5] | Accuracy: %83.0\n",
      "        - Testing with features [1 2 3 4 6] | Accuracy: %70.4\n",
      "        - Testing with features [1 2 3 5 6] | Accuracy: %85.0\n",
      "        - Testing with features [1 2 4 5 6] | Accuracy: %84.8\n",
      "        - Testing with features [1 3 4 5 6] | Accuracy: %84.8\n",
      "        - Testing with features [2 3 4 5 6] | Accuracy: %77.4\n",
      "[+] -- On level 1, we use feature subset of size 5: [1 2 3 5 6] with an accuracy of %85.0\n",
      "\n",
      "[::] -- Backward Elimination Search Tree Level 2 -- [::]\n",
      "        - Testing with features [1 2 3 4] | Accuracy: %71.8\n",
      "        - Testing with features [1 2 3 5] | Accuracy: %86.8\n",
      "        - Testing with features [1 2 3 6] | Accuracy: %71.4\n",
      "        - Testing with features [1 2 4 5] | Accuracy: %87.0\n",
      "        - Testing with features [1 2 4 6] | Accuracy: %71.8\n",
      "        - Testing with features [1 2 5 6] | Accuracy: %91.8\n",
      "        - Testing with features [1 3 4 5] | Accuracy: %86.4\n",
      "        - Testing with features [1 3 4 6] | Accuracy: %77.2\n",
      "        - Testing with features [1 3 5 6] | Accuracy: %89.2\n",
      "        - Testing with features [1 4 5 6] | Accuracy: %88.2\n",
      "        - Testing with features [2 3 4 5] | Accuracy: %78.0\n",
      "        - Testing with features [2 3 4 6] | Accuracy: %70.4\n",
      "        - Testing with features [2 3 5 6] | Accuracy: %80.8\n",
      "        - Testing with features [2 4 5 6] | Accuracy: %78.8\n",
      "        - Testing with features [3 4 5 6] | Accuracy: %81.6\n",
      "[+] -- On level 2, we use feature subset of size 4: [1 2 5 6] with an accuracy of %91.8\n",
      "\n",
      "[::] -- Backward Elimination Search Tree Level 3 -- [::]\n",
      "        - Testing with features [1 2 3] | Accuracy: %70.2\n",
      "        - Testing with features [1 2 4] | Accuracy: %70.4\n",
      "        - Testing with features [1 2 5] | Accuracy: %91.2\n",
      "        - Testing with features [1 2 6] | Accuracy: %73.0\n",
      "        - Testing with features [1 3 4] | Accuracy: %75.4\n",
      "        - Testing with features [1 3 5] | Accuracy: %91.4\n",
      "        - Testing with features [1 3 6] | Accuracy: %72.0\n",
      "        - Testing with features [1 4 5] | Accuracy: %93.4\n",
      "        - Testing with features [1 4 6] | Accuracy: %73.0\n",
      "        - Testing with features [1 5 6] | Accuracy: %94.4\n",
      "        - Testing with features [2 3 4] | Accuracy: %68.4\n",
      "        - Testing with features [2 3 5] | Accuracy: %81.4\n",
      "        - Testing with features [2 3 6] | Accuracy: %71.2\n",
      "        - Testing with features [2 4 5] | Accuracy: %80.8\n",
      "        - Testing with features [2 4 6] | Accuracy: %70.8\n",
      "        - Testing with features [2 5 6] | Accuracy: %85.2\n",
      "        - Testing with features [3 4 5] | Accuracy: %81.4\n",
      "        - Testing with features [3 4 6] | Accuracy: %73.0\n",
      "        - Testing with features [3 5 6] | Accuracy: %81.6\n",
      "        - Testing with features [4 5 6] | Accuracy: %84.4\n",
      "[+] -- On level 3, we use feature subset of size 3: [1 5 6] with an accuracy of %94.4\n",
      "\n",
      "[::] -- Backward Elimination Search Tree Level 4 -- [::]\n",
      "        - Testing with features [1 2] | Accuracy: %71.4\n",
      "        - Testing with features [1 3] | Accuracy: %73.6\n",
      "        - Testing with features [1 4] | Accuracy: %72.0\n",
      "        - Testing with features [1 5] | Accuracy: %96.2\n",
      "        - Testing with features [1 6] | Accuracy: %71.4\n",
      "        - Testing with features [2 3] | Accuracy: %72.6\n",
      "        - Testing with features [2 4] | Accuracy: %69.2\n",
      "        - Testing with features [2 5] | Accuracy: %83.2\n",
      "        - Testing with features [2 6] | Accuracy: %66.6\n",
      "        - Testing with features [3 4] | Accuracy: %70.8\n",
      "        - Testing with features [3 5] | Accuracy: %82.0\n",
      "        - Testing with features [3 6] | Accuracy: %69.0\n",
      "        - Testing with features [4 5] | Accuracy: %83.4\n",
      "        - Testing with features [4 6] | Accuracy: %68.8\n",
      "        - Testing with features [5 6] | Accuracy: %85.4\n",
      "[+] -- On level 4, we use feature subset of size 2: [1 5] with an accuracy of %96.2\n",
      "\n",
      "[::] -- Backward Elimination Search Tree Level 5 -- [::]\n",
      "        - Testing with features [1] | Accuracy: %72.4\n",
      "        - Testing with features [2] | Accuracy: %69.2\n",
      "        - Testing with features [3] | Accuracy: %68.0\n",
      "        - Testing with features [4] | Accuracy: %68.6\n",
      "        - Testing with features [5] | Accuracy: %85.6\n",
      "        - Testing with features [6] | Accuracy: %67.0\n",
      "[+] -- On level 5, we use feature subset of size 1: [5] with an accuracy of %85.6\n",
      "\n",
      "[:+:] ---  Backward Elimination Results: The best set of features is [1 5] | Accuracy: %96.2  --- [:+:]\n",
      "[1 5]\n"
     ]
    }
   ],
   "source": [
    "# @jit(target_backend='cuda')\n",
    "def generateCombos(data):\n",
    "    features= []\n",
    "    ftCombos= []\n",
    "    ftRow= data.shape[0]\n",
    "    for i in range(1,ftRow):\n",
    "        features.append(i)\n",
    "    for ft in features:\n",
    "        for combo in itertools.combinations(features, ft):\n",
    "            ftCombos.append(combo)\n",
    "    return (ftCombos)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "# @jit(target_backend='cuda')\n",
    "def generateDefaultRate(data):\n",
    "    classCnt= [0,0]\n",
    "    for pt in data[0]:\n",
    "        if pt==1:\n",
    "            classCnt[0]+= 1\n",
    "        else:\n",
    "            classCnt[1]+= 1\n",
    "    defaultRate= max(classCnt)/(classCnt[0]+classCnt[1])\n",
    "    return defaultRate\n",
    "\n",
    "\n",
    "# takes a single combo of features, classifies each dp by its nearest neighbor, returns a list of classifications for every dp\n",
    "@jit(target_backend='cuda')\n",
    "def classifier(data, ftSet):\n",
    "    rows= data.shape[1]\n",
    "    nearestNeighbors= np.empty(0)\n",
    "    for dp in range(0,rows):\n",
    "        nnIdx= 0\n",
    "        nearest= 10000\n",
    "        classification= 0\n",
    "        for  neighborIdx in range(0,rows):\n",
    "            if dp != neighborIdx:\n",
    "                dist= minkowski(data, dp, neighborIdx, ftSet, p=2)\n",
    "                if dist < nearest:\n",
    "                    nnIdx= neighborIdx\n",
    "                    nearest= dist\n",
    "        classification= data[0][nnIdx]\n",
    "        nearestNeighbors= np.append(nearestNeighbors, classification)\n",
    "    return nearestNeighbors\n",
    "\n",
    "\n",
    "#  Returns the accuracy of the classifier tested with a combination of features (leave-one-out, k=n)\n",
    "# @jit(target_backend='cuda')\n",
    "def crossValidate(data, ftSet):\n",
    "    classes= classifier(data, ftSet)\n",
    "    classesL= len(classes)\n",
    "    correct= 0\n",
    "    for j in range(0,classesL):\n",
    "        if classes[j] == data[0][j]:\n",
    "            correct+=1\n",
    "    accuracy = correct/classesL\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# @jit(target_backend='cuda')\n",
    "def forwardSelection(data, prevFtSet=np.asarray([]),finalAccuracy=0, finalSet=np.asarray([])):\n",
    "    ftSetLen= len(prevFtSet)\n",
    "    ftRowLength= data.shape[0]\n",
    "    if ftSetLen>= ftRowLength-1:\n",
    "        bestFtSet= finalSet\n",
    "        print(\"[:+:] ---  Forward Selection Results: The best set of features is \" +str(bestFtSet)+ \" | Accuracy: %\" +str(round(finalAccuracy*100,3))+ \"  --- [:+:]\")\n",
    "    else:\n",
    "        print(\"[::] -- Forward Selection Search Tree Level \"+str(ftSetLen+1)+ \" -- [::]\" )\n",
    "        if ftSetLen==0:\n",
    "            print(\"        - Testing without features  | Default Rate: %\" + str(round(generateDefaultRate(data)*100,3)))\n",
    "        bestAcc= 0\n",
    "        bestFtSubset= np.empty(0)\n",
    "        for ft in range(1, ftRowLength):\n",
    "            ftSet= np.empty(0)\n",
    "            if ft not in prevFtSet:\n",
    "                ftSet= np.append(prevFtSet, ft).astype(int)\n",
    "                ftAccuracy= crossValidate(data,ftSet)\n",
    "                print(\"        - Testing with features \"+ str(ftSet) + \" | Accuracy: %\" + str(round(ftAccuracy*100,3)))\n",
    "                if ftAccuracy>bestAcc: \n",
    "                    bestAcc= ftAccuracy\n",
    "                    bestFtSubset= ftSet\n",
    "        if bestAcc>finalAccuracy:\n",
    "            finalSet= bestFtSubset\n",
    "            finalAccuracy= bestAcc\n",
    "        print(\"[+] -- On level \"+str(ftSetLen+1)+ \", we add feature subset \" + str(bestFtSubset)+\" with an accuracy of %\" + str(round(bestAcc*100,3))+ \"\\n\")\n",
    "        bestFtSet=  forwardSelection(data, bestFtSubset,finalAccuracy, finalSet)\n",
    "    return bestFtSet\n",
    "\n",
    "\n",
    "\n",
    "def exhaustiveSelection(data):\n",
    "    bestAcc= 0\n",
    "    for combo in generateCombos(data):\n",
    "        ftSet= np.empty(0)\n",
    "        ftSet= np.append(ftSet, combo).astype(int)\n",
    "        ftAccuracy= crossValidate(data, ftSet )\n",
    "        print(\"        - Testing with features \"+ str(ftSet) + \" | Accuracy: %\" + str(round(ftAccuracy*100,3)))\n",
    "        if ftAccuracy>bestAcc:\n",
    "            bestAcc= ftAccuracy\n",
    "            bestFt= ftSet\n",
    "    print(\"[:+:] ---  Exaustive Selection Results: The best set of features is \" +str(bestFt)+ \" | Accuracy: %\" +str(round(bestAcc*100,3))+ \"  --- [:+:]\")\n",
    "\n",
    "def backwardElimination(data, ftCombos):\n",
    "    ftRowLength= data.shape[0]\n",
    "    iter= ftRowLength-1\n",
    "    dpCol= data.shape[1]\n",
    "    bestGlobalAcc= 0\n",
    "    while(iter>0):\n",
    "        level= ftRowLength-iter-1\n",
    "        bestLocalAcc= 0\n",
    "        print(\"[::] -- Backward Elimination Search Tree Level \"+str(level)+ \" -- [::]\" )\n",
    "        for combo in ftCombos:\n",
    "            ftSet= np.empty(0)\n",
    "            if len(combo)==iter:\n",
    "                ftSet= np.append(ftSet, combo).astype(int)\n",
    "                ftAccuracy= crossValidate(data, ftSet )\n",
    "                print(\"        - Testing with features \"+ str(ftSet) + \" | Accuracy: %\" + str(round(ftAccuracy*100,3)))\n",
    "                if ftAccuracy>bestLocalAcc:\n",
    "                    bestLocalAcc= ftAccuracy\n",
    "                    bestFt= ftSet\n",
    "        print(\"[+] -- On level \"+str(level)+ \", we use feature subset of size \"+ str(iter)+\": \" + str(bestFt)+\" with an accuracy of %\" + str(round(bestLocalAcc*100,3))+ \"\\n\")\n",
    "        if bestLocalAcc>bestGlobalAcc:\n",
    "            bestGlobalAcc=bestLocalAcc\n",
    "            bestGlobalFt= bestFt\n",
    "        iter-=1\n",
    "\n",
    "    print(\"[:+:] ---  Backward Elimination Results: The best set of features is \" +str(bestGlobalFt)+ \" | Accuracy: %\" +str(round(bestGlobalAcc*100,3))+ \"  --- [:+:]\")\n",
    "    return bestGlobalFt\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# start = timer()\n",
    "# print(forwardSelection(data))\n",
    "# print(\"Time without GPU:\", timer()-start)\n",
    "\n",
    "# print(backwardElimination(data, generateCombos(data)))\n",
    "\n",
    "# print(exhaustiveSelection(data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[:+:] Notes**\n",
    "-   The best single feature is #5, with an accuracy of 0.856"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Works Cited:\n",
    "\n",
    "\n",
    "-   https://www.analyticsvidhya.com/blog/2020/02/4-types-of-distance-metrics-in-machine-learning/\n",
    "\n",
    "-   https://www.geeksforgeeks.org/pandas-groupby-count-occurrences-in-column/\n",
    "-   https://www.codingem.com/python-how-to-get-all-combinations-of-a-list/\n",
    "-   https://www.geeksforgeeks.org/running-python-script-on-gpu/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d452713d07931529b46a6a4adbae9c48126cf3f4bc3a50be39ac3715dec6caaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
